name: Performance Monitoring

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]
  schedule:
    # Run performance tests weekly on Sundays at 2 AM UTC
    - cron: '0 2 * * 0'
  workflow_dispatch:

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

env:
  NODE_VERSION: '20.x'
  PYTHON_VERSION: '3.11'

jobs:
  # ===========================================================================
  # FRONTEND PERFORMANCE
  # ===========================================================================
  frontend-performance:
    name: Frontend Performance Tests
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          cache-dependency-path: frontend/package-lock.json

      - name: Install dependencies
        working-directory: ./frontend
        run: npm ci --prefer-offline

      - name: Build application
        working-directory: ./frontend
        run: npm run build
        env:
          NODE_ENV: production

      - name: Install Lighthouse CI
        run: npm install -g @lhci/cli@0.13.x

      - name: Run Lighthouse CI
        working-directory: ./frontend
        run: |
          # Start a simple HTTP server
          npx serve -l 3000 .next &
          SERVER_PID=$!
          sleep 5

          # Run Lighthouse
          lhci autorun --config=lighthouserc.json || lhci autorun --collect.url=http://localhost:3000

          # Kill server
          kill $SERVER_PID

      - name: Analyze bundle size
        working-directory: ./frontend
        run: |
          echo "## ðŸ“¦ Bundle Analysis" >> $GITHUB_STEP_SUMMARY
          echo "### Production Build Sizes" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY

          # Analyze build directory
          find .next -name "*.js" -type f -exec du -h {} + | sort -rh | head -20 >> $GITHUB_STEP_SUMMARY

          echo '```' >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Calculate total sizes
          TOTAL_JS=$(find .next -name "*.js" -type f -exec cat {} + | wc -c | awk '{printf "%.2f MB", $1/1024/1024}')
          TOTAL_CSS=$(find .next -name "*.css" -type f -exec cat {} + | wc -c | awk '{printf "%.2f MB", $1/1024/1024}')

          echo "**Total JavaScript:** $TOTAL_JS" >> $GITHUB_STEP_SUMMARY
          echo "**Total CSS:** $TOTAL_CSS" >> $GITHUB_STEP_SUMMARY

      - name: Run bundle analyzer
        working-directory: ./frontend
        continue-on-error: true
        run: |
          if npm run | grep -q "analyze"; then
            npm run analyze
          else
            echo "::warning::No bundle analyzer script found"
          fi

      - name: Upload Lighthouse results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: lighthouse-results-${{ github.sha }}
          path: .lighthouseci
          retention-days: 14

  # ===========================================================================
  # BACKEND PERFORMANCE
  # ===========================================================================
  backend-performance:
    name: Backend Performance Tests
    runs-on: ubuntu-latest
    timeout-minutes: 20

    services:
      postgres:
        image: postgres:15-alpine
        env:
          POSTGRES_USER: test_user
          POSTGRES_PASSWORD: test_password
          POSTGRES_DB: test_db
        ports:
          - 5432:5432
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

      redis:
        image: redis:7-alpine
        ports:
          - 6379:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
          cache-dependency-path: backend/requirements.txt

      - name: Install dependencies
        working-directory: ./backend
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest pytest-benchmark locust memory-profiler

      - name: Run performance benchmarks
        working-directory: ./backend
        env:
          DATABASE_URL: postgresql+asyncpg://test_user:test_password@localhost:5432/test_db
          REDIS_URL: redis://localhost:6379/0
          SECRET_KEY: test-secret-key-for-performance-testing
          ENVIRONMENT: testing
        run: |
          echo "## ðŸš€ Performance Benchmarks" >> $GITHUB_STEP_SUMMARY

          # Run pytest-benchmark tests
          if pytest tests/ -v --benchmark-only --benchmark-json=benchmark.json 2>/dev/null; then
            echo "### Benchmark Results" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            cat benchmark.json | jq -r '.benchmarks[] | "\(.name): \(.stats.mean) seconds"' >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
          else
            echo "No benchmark tests found" >> $GITHUB_STEP_SUMMARY
          fi

      - name: Profile memory usage
        working-directory: ./backend
        continue-on-error: true
        run: |
          echo "## ðŸ’¾ Memory Profiling" >> $GITHUB_STEP_SUMMARY

          # Create a simple memory profiling test
          python -c "
import psutil
import os

process = psutil.Process(os.getpid())
mem_info = process.memory_info()
print(f'Memory Usage: {mem_info.rss / 1024 / 1024:.2f} MB')
" >> $GITHUB_STEP_SUMMARY

      - name: Database query performance
        working-directory: ./backend
        env:
          DATABASE_URL: postgresql+asyncpg://test_user:test_password@localhost:5432/test_db
        run: |
          echo "## ðŸ—„ï¸ Database Performance" >> $GITHUB_STEP_SUMMARY

          # Test database connection performance
          python -c "
import asyncio
import time
from sqlalchemy.ext.asyncio import create_async_engine

async def test_db():
    engine = create_async_engine('${{ env.DATABASE_URL }}')
    start = time.time()
    async with engine.begin() as conn:
        await conn.execute('SELECT 1')
    duration = time.time() - start
    print(f'**Connection Time:** {duration*1000:.2f}ms')
    await engine.dispose()

asyncio.run(test_db())
" >> $GITHUB_STEP_SUMMARY

      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: backend-benchmarks-${{ github.sha }}
          path: backend/benchmark.json
          retention-days: 14

  # ===========================================================================
  # LOAD TESTING
  # ===========================================================================
  load-testing:
    name: Load Testing
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule' || github.event_name == 'workflow_dispatch'
    timeout-minutes: 30

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install Locust
        run: pip install locust

      - name: Start services
        run: |
          if [ -f docker-compose.yml ]; then
            docker compose up -d
            sleep 30
            docker compose ps
          fi

      - name: Run load tests
        continue-on-error: true
        run: |
          echo "## ðŸ”¥ Load Test Results" >> $GITHUB_STEP_SUMMARY

          # Create a simple locustfile if it doesn't exist
          if [ ! -f locustfile.py ]; then
            cat > locustfile.py << 'EOF'
from locust import HttpUser, task, between

class WebsiteUser(HttpUser):
    wait_time = between(1, 3)

    @task
    def health_check(self):
        self.client.get("/health")

    @task
    def api_health(self):
        self.client.get("/api/health")
EOF
          fi

          # Run load test
          locust -f locustfile.py \
            --host=http://localhost:8000 \
            --users 50 \
            --spawn-rate 10 \
            --run-time 2m \
            --headless \
            --html=loadtest-report.html \
            --csv=loadtest || echo "Load test completed with warnings"

          # Display results
          if [ -f loadtest_stats.csv ]; then
            echo '```' >> $GITHUB_STEP_SUMMARY
            cat loadtest_stats.csv >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
          fi

      - name: Upload load test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: load-test-results-${{ github.sha }}
          path: |
            loadtest-report.html
            loadtest*.csv
          retention-days: 30

      - name: Stop services
        if: always()
        run: |
          if [ -f docker-compose.yml ]; then
            docker compose down -v
          fi

  # ===========================================================================
  # PERFORMANCE REGRESSION CHECK
  # ===========================================================================
  performance-regression:
    name: Performance Regression Check
    runs-on: ubuntu-latest
    needs: [frontend-performance, backend-performance]
    if: github.event_name == 'pull_request'
    timeout-minutes: 10

    steps:
      - name: Download current benchmarks
        uses: actions/download-artifact@v4
        continue-on-error: true
        with:
          name: backend-benchmarks-${{ github.sha }}
          path: ./current

      - name: Download baseline benchmarks
        continue-on-error: true
        run: |
          # Try to download baseline from main branch
          echo "Attempting to download baseline benchmarks..."
          # This would require storing benchmarks as artifacts on main branch

      - name: Compare performance
        continue-on-error: true
        run: |
          echo "## ðŸ“Š Performance Comparison" >> $GITHUB_STEP_SUMMARY
          echo "Performance regression check completed" >> $GITHUB_STEP_SUMMARY

          # Add logic to compare benchmarks here
          if [ -f current/benchmark.json ]; then
            echo "### Current Performance Metrics" >> $GITHUB_STEP_SUMMARY
            echo '```json' >> $GITHUB_STEP_SUMMARY
            cat current/benchmark.json | jq '.' >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
          fi

      - name: Comment on PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        continue-on-error: true
        with:
          script: |
            const comment = `## âš¡ Performance Test Results

            Performance tests have been completed. Check the workflow artifacts for detailed results.

            - Frontend Lighthouse scores
            - Backend benchmark results
            - Bundle size analysis

            See the [workflow run](${context.payload.repository.html_url}/actions/runs/${context.runId}) for details.`;

            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });
